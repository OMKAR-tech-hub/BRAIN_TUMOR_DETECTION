ğŸ§  Brain Tumor Detection using CNN

This project builds a Convolutional Neural Network (CNN) model using TensorFlow and Keras to automatically detect brain tumors from MRI images.

The model is trained on a Brain MRI dataset sourced from Kaggle, which can be found here
.

ğŸ“ About the Dataset

The dataset includes two folders:

yes/ â†’ 155 MRI images containing brain tumors

no/ â†’ 98 MRI images without tumors

In total, there are 253 MRI scans used for model training and validation.

ğŸš€ Getting Started

âš ï¸ Sometimes GitHub doesnâ€™t render Jupyter notebooks properly.
You can view them easily using nbviewer
.

ğŸ§© Data Augmentation
Why Augmentation Was Used

Since the dataset is quite small and imbalanced, data augmentation was performed to generate more diverse examples. This helps the CNN generalize better and prevents overfitting.

Dataset Summary
Stage	Tumorous Images	Non-Tumorous Images	Total
Before Augmentation	155	98	253
After Augmentation	1085	980	2065

All augmented samples are stored inside the â€˜augmented dataâ€™ directory, including the original 253 images.

ğŸ§¼ Data Preprocessing

Each MRI image underwent the following preprocessing steps:

Brain Region Cropping â€“ Extract only the brain portion from the image to eliminate background noise.

Resizing â€“ Resize all images to (240, 240, 3) to maintain consistent input dimensions.

Normalization â€“ Scale pixel values between 0 and 1 for better convergence during training.

ğŸ”€ Data Splitting

The processed dataset was divided into:

70% â†’ Training set

15% â†’ Validation set

15% â†’ Test set

ğŸ§  Model Architecture

The model follows a simple CNN architecture optimized for small datasets and limited compute power.

Layers Overview

ZeroPadding2D with pool size (2, 2)

Conv2D layer (32 filters, kernel size (7,7), stride 1)

BatchNormalization

ReLU Activation

MaxPooling2D with pool size (4,4)

MaxPooling2D (again)

Flatten Layer

Dense Layer with 1 output neuron (Sigmoid activation for binary classification)

âš™ï¸ Why This Architecture?

Initially, pre-trained models like VGG16 and ResNet50 were tested, but due to limited data and hardware (Intel i7 6th Gen, 8GB RAM), they led to overfitting and long training times.
Hence, a lightweight CNN was built and trained from scratch â€” achieving competitive results.

ğŸ“Š Model Training

The model was trained for 24 epochs with binary cross-entropy loss and Adam optimizer.

Training Visualizations:




The best validation accuracy was achieved on epoch 23.

ğŸ§¾ Results
Metric	Validation Set	Test Set
Accuracy	91%	89%
F1 Score	0.91	0.88

âœ… Final Test Accuracy: 88.7%
âœ… Final F1 Score: 0.88

These metrics demonstrate that the model performs well even with limited and balanced data.

ğŸ’¾ Model Files

All model weights and checkpoints are stored in the models/ folder.

The best model is saved as:

cnn-parameters-improvement-23-0.91.model


You can easily reload it using:

from tensorflow.keras.models import load_model
best_model = load_model('models/cnn-parameters-improvement-23-0.91.model')

ğŸ“‚ Repository Structure
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ yes/
â”‚   â”œâ”€â”€ no/
â”‚   â””â”€â”€ augmented data/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ cnn-parameters-improvement-23-0.91.model
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ preprocessing_and_training.ipynb
â”œâ”€â”€ Loss.PNG
â”œâ”€â”€ Accuracy.PNG
â””â”€â”€ README.md

ğŸ¤ Contributions

Feel free to open issues or submit pull requests for further improvements such as:

Using advanced architectures like EfficientNet or MobileNet

Hyperparameter tuning

Visualizing Grad-CAM heatmaps

âœ¨ Acknowledgments

Special thanks to the original dataset contributors and TensorFlow community for tools and resources that made this project possible.